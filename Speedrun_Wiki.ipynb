{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/EmirhanEge1/Wikipedia-Speedrunner/blob/main/Speedrun_Wiki.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sh4UlwUze0h0"
      },
      "outputs": [],
      "source": [
        "!pip install -U sentence-transformers wikipedia-api"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TV_qXzsZe6YQ"
      },
      "outputs": [],
      "source": [
        "import wikipediaapi\n",
        "import time\n",
        "import heapq\n",
        "import torch\n",
        "import pandas as pd\n",
        "import random\n",
        "import re\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2').to(device)\n",
        "wiki_en = wikipediaapi.Wikipedia(\n",
        "    user_agent='DataEngineerBot/1.0 (email@example.com)',\n",
        "    language='en',\n",
        "    extract_format=wikipediaapi.ExtractFormat.WIKI\n",
        ")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F_IwRDAqjEr0"
      },
      "outputs": [],
      "source": [
        "def is_valid_link(link):\n",
        "    exclude = [\"Category:\", \"File:\", \"Help:\", \"Template:\", \"Talk:\", \"Portal:\", \"Special:\", \"Wikipedia:\"]\n",
        "    return not any(link.startswith(prefix) for prefix in exclude)\n",
        "\n",
        "def fast_pre_filter(links, target_page, top_n=50):\n",
        "    target_words = set(re.findall(r'\\w+', target_page.lower()))\n",
        "    scored_links = []\n",
        "\n",
        "    for link in links:\n",
        "        score = 0\n",
        "        link_lower = link.lower()\n",
        "        for word in target_words:\n",
        "            if word in link_lower:\n",
        "                score += 1\n",
        "        scored_links.append((score, link))\n",
        "\n",
        "    scored_links.sort(key=lambda x: x[0], reverse=True)\n",
        "    return [link for score, link in scored_links[:top_n]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jvxLXsANFP26"
      },
      "outputs": [],
      "source": [
        "def start_wikipedia_speedrunner(start_page, target_page, timeout=60):\n",
        "    start_time = time.perf_counter()\n",
        "    target_emb = model.encode(target_page, convert_to_tensor=True, normalize_embeddings=True)\n",
        "    target_keywords = [w.lower() for w in target_page.split() if len(w) > 2]\n",
        "\n",
        "    queue = [(-1.0, start_page, [start_page])]\n",
        "    visited = {start_page.lower()}\n",
        "    step_count = 0\n",
        "\n",
        "    while queue:\n",
        "        elapsed = time.perf_counter() - start_time\n",
        "        if elapsed > timeout:\n",
        "            return {\"steps\": 0, \"explored\": step_count, \"success\": False, \"status\": \"Timed Out\", \"duration\": elapsed, \"path\": []}\n",
        "\n",
        "        neg_score, current_title, path = heapq.heappop(queue)\n",
        "        step_count += 1\n",
        "\n",
        "        if current_title.lower() == target_page.lower():\n",
        "            return {\"steps\": len(path)-1, \"explored\": step_count, \"success\": True, \"status\": \"Success\", \"duration\": elapsed, \"path\": path}\n",
        "\n",
        "        try:\n",
        "            time.sleep(0.04)\n",
        "            page = wiki_en.page(current_title)\n",
        "            if not page.exists(): continue\n",
        "            all_links = list(page.links.keys())\n",
        "        except Exception as e:\n",
        "            time.sleep(1.0)\n",
        "            continue\n",
        "\n",
        "        raw_links = [l for l in all_links if l.lower() not in visited and is_valid_link(l)]\n",
        "        turbo_links = fast_pre_filter(raw_links, target_page, top_n=50)\n",
        "\n",
        "        if not turbo_links: continue\n",
        "\n",
        "        try:\n",
        "            link_embs = model.encode(turbo_links, convert_to_tensor=True, normalize_embeddings=True, batch_size=128)\n",
        "            cos_scores = util.dot_score(target_emb, link_embs)[0]\n",
        "        except: continue\n",
        "\n",
        "        for i, link_name in enumerate(turbo_links):\n",
        "            link_score = cos_scores[i].item()\n",
        "            link_lower = link_name.lower()\n",
        "\n",
        "            for kw in target_keywords:\n",
        "                if kw in link_lower: link_score += 0.5\n",
        "\n",
        "\n",
        "\n",
        "            if i < 15: link_score += 0.05\n",
        "\n",
        "            link_score -= (len(path) * 0.02)\n",
        "\n",
        "            if link_lower not in visited:\n",
        "                visited.add(link_lower)\n",
        "                heapq.heappush(queue, (-link_score, link_name, path + [link_name]))\n",
        "\n",
        "    return {\"steps\": 0, \"explored\": step_count, \"success\": False, \"status\": \"No Path\", \"duration\": time.perf_counter() - start_time, \"path\": []}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U33eivSjFWN1"
      },
      "outputs": [],
      "source": [
        "start_wikipedia_speedrunner( \"Mushroom\" ,\"Walmart\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iUGFIePcM03B"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "Modes:\n",
        "- 'vital': Targets the top 1,000/10,000 essential articles.\n",
        "- 'academic': Focuses on academic disciplines and their specialized branches.\n",
        "- 'deep': Scrapes links from the deeper layers of a random topic.\n",
        "\"\"\"\n",
        "\n",
        "def run_benchmark(num_tests=50, mode=\"vital\"):\n",
        "    print(f\"\\nMode='{mode}'...\")\n",
        "    pool = []\n",
        "\n",
        "    if mode == \"vital\":\n",
        "        page = wiki_en.page(\"Wikipedia:Vital articles\")\n",
        "        pool = [l for l in page.links.keys() if is_valid_link(l)]\n",
        "    elif mode == \"academic\":\n",
        "        hubs = [\"Outline of academic disciplines\", \"List of academic fields\"]\n",
        "        for hub in hubs:\n",
        "            p = wiki_en.page(hub)\n",
        "            pool.extend([l for l in p.links.keys() if is_valid_link(l)])\n",
        "        pool = list(set(pool))\n",
        "    elif mode == \"deep\":\n",
        "        base_topics = [\n",
        "    \"Science\", \"History\", \"Technology\", \"Arts\",\n",
        "    \"Geography\", \"Philosophy\", \"Religion\", \"Mathematics\",\n",
        "    \"Biology\", \"Medicine\", \"Economics\", \"Law\",\n",
        "    \"Literature\", \"Music\", \"Politics\", \"Environment\",\n",
        "    \"Psychology\", \"Sociology\", \"Sports\", \"Engineering\"\n",
        "]\n",
        "        chosen = wiki_en.page(random.choice(base_topics))\n",
        "        first_tier = random.sample(list(chosen.links.keys()), min(30, len(chosen.links)))\n",
        "        for link in first_tier:\n",
        "            try:\n",
        "                p = wiki_en.page(link)\n",
        "                sub_links = list(p.links.keys())\n",
        "                pool.extend(random.sample(sub_links, min(10, len(sub_links))))\n",
        "            except: continue\n",
        "        pool = [l for l in pool if is_valid_link(l)]\n",
        "\n",
        "    results = []\n",
        "    print(f\" {mode.upper()} Benchmark: {num_tests} \")\n",
        "    print(\"=\" * 110)\n",
        "\n",
        "    for i in range(num_tests):\n",
        "        try:\n",
        "            start, end = random.sample(pool, 2)\n",
        "        except ValueError: break\n",
        "\n",
        "        print(f\"[{i+1:03d}/{num_tests}] {start[:25]:<25} âžœ {end[:25]:<25}\", end=\" | \", flush=True)\n",
        "\n",
        "        res = start_wikipedia_speedrunner(start, end, timeout=60)\n",
        "\n",
        "        status = res.get(\"status\", \"Error\")\n",
        "        duration = round(res.get(\"duration\", 0), 2)\n",
        "        steps = res.get(\"steps\", 0)\n",
        "\n",
        "        current_res = {\n",
        "            \"Start\": start, \"End\": end,\n",
        "            \"Steps\": steps, \"Explored\": res.get(\"explored\", 0),\n",
        "            \"Duration\": duration, \"Status\": status,\n",
        "            \"Path\": \" âžœ \".join(res[\"path\"]) if res.get(\"success\") else \"\"\n",
        "        }\n",
        "        results.append(current_res)\n",
        "\n",
        "        icon = \"âœ…\" if res.get(\"success\") else \"âŒ\"\n",
        "        print(f\"{icon} {status:<10} | {duration:>5.2f}s | AdÄ±m: {steps}\")\n",
        "\n",
        "        if res.get(\"success\"):\n",
        "            print(f\"Route: {current_res['Path']}\")\n",
        "\n",
        "        if (i + 1) % 10 == 0:\n",
        "            time.sleep(3)\n",
        "            pd.DataFrame(results).to_csv(f\"benchmark_{mode}_interim.csv\", index=False)\n",
        "\n",
        "    df = pd.DataFrame(results)\n",
        "    df.to_csv(f\"benchmark_{mode}_final.csv\", index=False)\n",
        "\n",
        "    if not df.empty:\n",
        "        success_rate = (len(df[df.Status == \"Success\"]) / len(df)) * 100\n",
        "        print(\"=\" * 110)\n",
        "        print(f\"ðŸ“Š BAÅžARI ORANI: %{success_rate:.2f} | ORT. SÃœRE: {df['Duration'].mean():.2f}s\")\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yvjIi5AaPeuh"
      },
      "outputs": [],
      "source": [
        "df_vital = run_benchmark(num_tests=300, mode=\"vital\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_academic = run_benchmark(num_tests=300, mode=\"academic\")"
      ],
      "metadata": {
        "id": "GbbDqZThqQpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_vital = run_benchmark(num_tests=300, mode=\"deep\")"
      ],
      "metadata": {
        "id": "usXbA2aXqQ3s"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOx+ehfJko8k907JOULF3ZM",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}